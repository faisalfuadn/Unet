# -*- coding: utf-8 -*-
"""Unet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlrNxDAboGMYXTR2DUHmBuicTPmzZ11M
"""

import tensorflow as tf
from tensorflow.keras import layers, models, backend as K

# Basic conv block with residual connection
def conv_block(x, filters, act='relu', kernel_size=(3,3), padding='same', dropout=0.0):
    shortcut = layers.Conv2D(filters, (1,1), padding=padding)(x)
    x = layers.Conv2D(filters, kernel_size, activation=act, padding=padding)(x)
    x = layers.Conv2D(filters, kernel_size, activation=act, padding=padding)(x)
    if dropout > 0:
        x = layers.Dropout(dropout)(x)
    x = layers.Add()([x, shortcut])
    return x

# Attention gate
def attention_gate(x, g, filters):
    theta_x = layers.Conv2D(filters, (1,1), padding='same')(x)
    phi_g = layers.Conv2D(filters, (1,1), padding='same')(g)
    add = layers.Activation('relu')(layers.add([theta_x, phi_g]))
    psi = layers.Conv2D(1, (1,1), activation='sigmoid', padding='same')(add)
    return layers.multiply([x, psi])

# Attention Residual U-Net++
def AttentionResidualUNetPP(img_size=(256,256), n_labels=1, nb_filters=32, depth=4, act='relu', dropout=0.0):
    inputs = layers.Input((*img_size, 3))
    skips = []
    x = inputs

    # Encoder path
    for i in range(depth):
        x = conv_block(x, nb_filters, act=act, dropout=dropout)
        skips.append(x)
        x = layers.MaxPooling2D((2,2))(x)
        nb_filters *= 2

    # Bottleneck
    x = conv_block(x, nb_filters, act=act, dropout=dropout)

    # Decoder path with attention and residual dense skip
    for i in reversed(range(depth)):
        nb_filters //= 2
        x = layers.UpSampling2D((2,2))(x)
        att = attention_gate(skips[i], x, nb_filters)
        x = layers.concatenate([att, skips[i], x])
        x = conv_block(x, nb_filters, act=act, dropout=dropout)

    outputs = layers.Conv2D(n_labels, (1,1), activation='sigmoid')(x)
    model = models.Model(inputs, outputs)
    return model

# Dice coefficient and loss
def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coef(y_true, y_pred)

# Compile suggestion for binary segmentation
def compile_model(model, binary=True):
    if binary:
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                      loss=dice_loss,
                      metrics=['accuracy', dice_coef])
    else:
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])
    return model

# Example training loop for medical segmentation
def train_model(model, train_dataset, val_dataset, epochs=50):
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_dice_coef', mode='max'),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    ]

    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=epochs,
        callbacks=callbacks
    )
    return history

# Example usage
if __name__ == '__main__':
    model = AttentionResidualUNetPP(img_size=(128,128), n_labels=1)
    model = compile_model(model, binary=True)
    model.summary()